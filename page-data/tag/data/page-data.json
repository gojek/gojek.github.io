{"componentChunkName":"component---src-templates-tag-js","path":"/tag/data/","result":{"data":{"ghostTag":{"slug":"data","name":"Data","visibility":"public","feature_image":null,"description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","meta_title":null,"meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5eb0f8af5524cd001e7391f4","title":"How Gojek Uses NLP to Name Pickup Locations at Scale","slug":"how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2","featured":false,"feature_image":"https://res-2.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_dFalBje-vQCkEY8Zrq9P5g.jpg","excerpt":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","custom_excerpt":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"01 May, 2020","updated_at_pretty":"12 May, 2020","created_at":"2020-05-05T10:55:03.000+05:30","published_at":"2020-05-01T09:30:00.000+05:30","updated_at":"2020-05-12T11:51:38.000+05:30","meta_title":"How Gojek Uses NLP to Name Pickup Points at Scale","meta_description":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"When our customers want to use our ride hailing products like GoRide and GoCar,\nthey are presented with convenient, clearly named pickup points nearby. Here’s\nan example:\n\nThis saves customers the hassle of calling the driver partner, explaining where\nthey are, what colour clothes they are wearing, and so on. Our pickup points are\ndesigned to make lives easier for both customers and driver partners.\n\nThis is possible because the pickup points shown on the app are popular pickup\nlocations around the area. What’s more, the pickup point names are displayed\nexactly how customers driver partners usually refer to them.\n\nBut how do we manage to name so many pickup points accurately, and at scale?\n\nWe use past booking locations and their associated chat logs to discover named\npickup points. As our previous research has explained, we first perform \nclustering\n[https://blog.gojekengineering.com/fantastic-drivers-and-how-to-find-them-a88239ef3b29] \non historical bookings to form potential pickup points, then we use a language\nmodel\n[https://blog.gojekengineering.com/how-i-met-my-gojek-driver-without-a-single-call-95041f4fdd03] \nto select the best name. Here, we explain how we improved upon the previous\nstatistical language model with a state-of-the-art NLP model, which makes the\nentire naming exercise fully scalable. This is the magic behind all the pickup\npoints seen on the Gojek app.\n\nHow can we learn better?\nAs explained in our previous post, our original statistical language model\nselects the best pickup point name from the most probable n-grams extracted from\nbookings text. However, such a statistical language model doesn’t ‘understand’\nthe meaning of the texts, it simply chooses phrases with high frequencies\nwithout knowing the semantics. Sometimes it throws street names, sometimes even\ncommon phrases with no information about location. We have to manually check\neverything to make sure it reflects the right POI, before it appears on the app.\n\nThis creates a challenge — especially if we want to quickly expand the\nfrictionless pickup experience to customers across in new geographies. Hence, we\ndecided to go a step further with a deep-learning NLP model that ‘understands’\nand ‘learns’ to differentiate what is a valid pickup point name.\n\nAt Gojek, we never stop thinking and always go a step further\n\nMeet CartoBERT ?\nOne of the most recent and impactful breakthroughs NLP was the publication of\nBERT[1] — a contextual language representation with transformer models — by\nGoogle in late 2018. It obtained state-of-the-art results on a wide array of NLP\ntasks. In the 2019, many NLP researches were influenced by BERT, including\nXLNet, RoBERTa, ERNIE etc.\n\nBERT Explained\nBERT, or Bidirectional Encoder Representations from Transformers, is composed of\nan embedding layer, followed by groups of transformer layers.\n\nEvery word (token) in the input sentence will first get encoded into its\nembedding representations in the embedding layer, and then go through\nbidirectional transformer encoder layers. Every encoder layer will perform the\nmulti-head attention computation on the token representation from the previous\nlayer to create a new intermediate representation, which is then output to the\nnext layer. The output from the final layer is the contextual representation of\nthe input token. A pooled sentence level representation combining all token\nrepresentations could be created if needed by specific downstream tasks.\n\nWith the final contextual representations at either token or sentence level, a\npre-trained BERT on large unlabelled text corpus, could be further extended to a\nwide variety of NLP tasks, such as text classification, question answering,\nNamed Entity Recognition (NER) etc.\n\nALBERT[2], published by Google in Sep 2019, improved on BERT with embedding\nparameter factorisation and cross layer parameter sharing to reduce the number\nof parameters (by 9 times for base model). It also uses sequence order\nprediction instead of next sentence prediction for the pre-train task. In the\npaper, ALBERT also outperforms BERT on standard NLP tasks/datasets (SQUAD, RACE\netc), with fewer parameters.\n\nPre-train CartoBERT to learn language representation from Gojek bookings text\nInspired by ALBERT’s lightweight model and performance, we developed CartoBERT,\nGojek’s very own pickup point name recognition model, based on ALBERT’s\narchitecture.\n\nAs illustrated below, the uncased CartoBERT is pre-trained on Gojek’s own masked\nbookings text corpus of about 200 million sentences. Booking text is first\npre-processed for data masking to mask all customer sensitive information,\nlanguage detection, text normalisation (including text cleaning, slang,\nabbreviation transformations, lowercase transformation and emoji removal). The\npre-processed text is used to build subword vocabularies which handles\nOut-Of-Vocabulary (OOV) tokens that could be decomposed to frequent subword\npatterns. CartoBERT tokenizer is then created with the subword vocabularies and\nfurther used to encode and tokenize the same preprocessed bookings text to form\npre-trained input files.\n\nSame as ALBERT, the model is pre-trained to ‘understand’ Gojek’s bookings text\nusing Masked Language Model — which predicts randomly masked tokens in input\nsentences — and Sentence Order Prediction tasks, which predicts the order of\ninput sentences pair.\n\nFine-tuning CartoBERT to extract pickup point names from Gojek bookings text\nWith the huge amount of bookings text we have at Gojek, now CartoBERT can better\n‘understand’ past bookings text. Theoretically, it ‘understands’ every word of a\nbooking text sentence.\n\nFor every token in the input sentence, CartoBERT will output a 768-dimension\nvector (we use the default hidden layer size of the ALBERT base model in\nCartoBERT, however this is configurable) from last transformer encoder layer,\nand we use that to represent CartoBERT’s ‘understanding’ of the token’s meaning\nin the sentence context for fine-tune step.\n\nAs illustrated in the diagram below, while fine-tuning CartoBERT for pickup\npoint name recognition, we replace the Masked Language Model and Sequence Order\nPrediction layers from CartoBERT in pre-train step with token classification\nlayer. The token classification layer learns to predict the probability of a\ntoken belonging to a pickup point name, with the final token representation\noutput from CartoBERT transformer layers, from labelled training data created\nwith bookings text sentences, and corresponding pickup point names. Here, we use\nweighted cross entropy loss to deal with class imbalance, as tokens tagged to\npickup point names are a minority.\n\n\n\nWith this, CartoBERT is fine-tuned to extract pickup point names from bookings\ntext sentences, if any.\n\nHow does the model perform?\nCartoBERT gives a lift of more than 25% in pickup point name accuracy to ~93%\naccuracy, which is measured as the percentage of valid pick up point names out\nof generated names. With this high accuracy, we have achieved full scalability\nof automatic generation for named pickup points to quickly cover multiple\ngeographies without heavy reliance on human inputs.\n\nWhat’s next?\nWe are not stopping here and are exploring using active learning to further\nimprove CartoBERT. With active learning, we only flag out uncertain predictions,\nwhich are measured as sentence level least token probability[3] for human\nlabelling. We then use human-curated data as feedback for model learning. In\nthis way, we can improve model learning efficiency with minimum labelling\neffort.\n\nWhat’s more, with the success of CartoBERT, we are considering pre-training and\nopen sourcing a general Indonesia Bahasa ALBERT model with Indonesia open corpus\nfrom wiki, news, Twitter etc. Currently, the options for open-sourced language\nmodel in Indonesia Bahasa are very limited, only pre-trained static word\nembeddings such as word2vec, fasttext etc are available. It would be beneficial\nto the community if we have a good state-of-the-art attention-based transformer\nmodel for the language. Stay tuned for more updates from the Cartography Data\nScience team. ?\n\nLeave a ? if you liked what you read. Ping me with suggestions and feedback.\n\nThanks to all the amazing people who contributed to this post: Tan Funan, Zane\nLim, Dang Le, Lijuan Chia, Bani Widyatmiko, Maureen Koha, Ringga Saputra, Nur\nIzzahudinr, Sandya Ardi, Yeni Primasari, Ardya Dipta.\n\n\n--------------------------------------------------------------------------------\n\nReferences\n\n[1] J. Devlin [https://arxiv.org/search/cs?searchtype=author&query=Devlin%2C+J], \nM. Chang [https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M], K.\nLee [https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K], K. Toutanova\n[https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K]: BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [https://arxiv.org/abs/1810.04805] (2018)\n\n[2] Z. Lan [https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Z], M.\nChen [https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M], S. Goodman\n[https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S], K. Gimpel\n[https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K], P. Sharma\n[https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+P], R. Soricut\n[https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R]: ALBERT: A\nLite BERT for Self-supervised Learning of Language Representations. \narXiv:1909.11942 [https://arxiv.org/abs/1909.11942] (2019)\n\n[3] M.Liu [https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M], Z. Tu\n[https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z], Z. Wang\n[https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z], X. Xu\n[https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X]: LTP: A New Active\nLearning Strategy for Bert-CRF Based Named Entity Recognition. arXiv:2001.02524\n[https://arxiv.org/abs/2001.02524] (2020)\n\n\n--------------------------------------------------------------------------------\n\nLiked what you read? Sign up for our newsletter\n[https://mailchi.mp/go-jek/gojek-tech-newsletter] to have our latest stories\ndelivered straight to your inbox!","html":"<p>When our customers want to use our ride hailing products like GoRide and GoCar, they are presented with convenient, clearly named pickup points nearby. Here’s an example:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/180/1*msS7z4IN06LVM0XvClmmPQ.gif\" class=\"kg-image\"></figure><p>This saves customers the hassle of calling the driver partner, explaining where they are, what colour clothes they are wearing, and so on. Our pickup points are designed to make lives easier for both customers and driver partners.</p><p>This is possible because the pickup points shown on the app are popular pickup locations around the area. What’s more, the pickup point names are displayed exactly how customers driver partners usually refer to them.</p><p><strong><strong>But how do we manage to name so many pickup points accurately, and at scale?</strong></strong></p><p>We use past booking locations and their associated chat logs to discover named pickup points. As our previous research has explained, we first perform <a href=\"https://blog.gojekengineering.com/fantastic-drivers-and-how-to-find-them-a88239ef3b29\" rel=\"noopener\">clustering</a> on historical bookings to form potential pickup points, then we use a <a href=\"https://blog.gojekengineering.com/how-i-met-my-gojek-driver-without-a-single-call-95041f4fdd03\" rel=\"noopener\">language model</a> to select the best name. Here, we explain how we improved upon the previous statistical language model with a state-of-the-art NLP model, which makes the entire naming exercise fully scalable. This is the magic behind all the pickup points seen on the Gojek app.</p><h1 id=\"how-can-we-learn-better\">How can we learn better?</h1><p>As explained in our previous post, our original statistical language model selects the best pickup point name from the most probable n-grams extracted from bookings text. However, such a statistical language model doesn’t ‘understand’ the meaning of the texts, it simply chooses phrases with high frequencies without knowing the semantics. Sometimes it throws street names, sometimes even common phrases with no information about location. We have to manually check everything to make sure it reflects the right POI, before it appears on the app.</p><p>This creates a challenge — especially if we want to quickly expand the frictionless pickup experience to customers across in new geographies. Hence, we decided to go a step further with a deep-learning NLP model that ‘understands’ and ‘learns’ to differentiate what is a valid pickup point name.</p><p><em><em>At Gojek, we never stop thinking and always go a step further</em></em></p><h1 id=\"meet-cartobert-\">Meet CartoBERT ?</h1><p>One of the most recent and impactful breakthroughs NLP was the publication of BERT[1] — a contextual language representation with transformer models — by Google in late 2018. It obtained state-of-the-art results on a wide array of NLP tasks. In the 2019, many NLP researches were influenced by BERT, including XLNet, RoBERTa, ERNIE etc.</p><h2 id=\"bert-explained\">BERT Explained</h2><p>BERT, or Bidirectional Encoder Representations from Transformers, is composed of an embedding layer, followed by groups of transformer layers.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/1226/1*qHFXdPcW_3UkLEsRJy2FRg.png\" class=\"kg-image\"></figure><p>Every word (token) in the input sentence will first get encoded into its embedding representations in the embedding layer, and then go through bidirectional transformer encoder layers. Every encoder layer will perform the multi-head attention computation on the token representation from the previous layer to create a new intermediate representation, which is then output to the next layer. The output from the final layer is the contextual representation of the input token. A pooled sentence level representation combining all token representations could be created if needed by specific downstream tasks.</p><p>With the final contextual representations at either token or sentence level, a pre-trained BERT on large unlabelled text corpus, could be further extended to a wide variety of NLP tasks, such as text classification, question answering, Named Entity Recognition (NER) etc.</p><p>ALBERT[2], published by Google in Sep 2019, improved on BERT with embedding parameter factorisation and cross layer parameter sharing to reduce the number of parameters (by 9 times for base model). It also uses sequence order prediction instead of next sentence prediction for the pre-train task. In the paper, ALBERT also outperforms BERT on standard NLP tasks/datasets (SQUAD, RACE etc), with fewer parameters.</p><h2 id=\"pre-train-cartobert-to-learn-language-representation-from-gojek-bookings-text\">Pre-train CartoBERT to learn language representation from Gojek bookings text</h2><p>Inspired by ALBERT’s lightweight model and performance, we developed CartoBERT, Gojek’s very own pickup point name recognition model, based on ALBERT’s architecture.</p><p>As illustrated below, the uncased CartoBERT is pre-trained on Gojek’s own masked bookings text corpus of about 200 million sentences. Booking text is first pre-processed for data masking to mask all customer sensitive information, language detection, text normalisation (including text cleaning, slang, abbreviation transformations, lowercase transformation and emoji removal). The pre-processed text is used to build subword vocabularies which handles Out-Of-Vocabulary (OOV) tokens that could be decomposed to frequent subword patterns. CartoBERT tokenizer is then created with the subword vocabularies and further used to encode and tokenize the same preprocessed bookings text to form pre-trained input files.</p><p>Same as ALBERT, the model is pre-trained to ‘understand’ Gojek’s bookings text using Masked Language Model — which predicts randomly masked tokens in input sentences — and Sentence Order Prediction tasks, which predicts the order of input sentences pair.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1386/1*STaqWssxlYPhFaDulLwJzg.png\" class=\"kg-image\"></figure><h2 id=\"fine-tuning-cartobert-to-extract-pickup-point-names-from-gojek-bookings-text\">Fine-tuning CartoBERT to extract pickup point names from Gojek bookings text</h2><p>With the huge amount of bookings text we have at Gojek, now CartoBERT can better ‘understand’ past bookings text. Theoretically, it ‘understands’ every word of a booking text sentence.</p><p>For every token in the input sentence, CartoBERT will output a 768-dimension vector (we use the default hidden layer size of the ALBERT base model in CartoBERT, however this is configurable) from last transformer encoder layer, and we use that to represent CartoBERT’s ‘understanding’ of the token’s meaning in the sentence context for fine-tune step.</p><p>As illustrated in the diagram below, while fine-tuning CartoBERT for pickup point name recognition, we replace the Masked Language Model and Sequence Order Prediction layers from CartoBERT in pre-train step with token classification layer. The token classification layer learns to predict the probability of a token belonging to a pickup point name, with the final token representation output from CartoBERT transformer layers, from labelled training data created with bookings text sentences, and corresponding pickup point names. Here, we use weighted cross entropy loss to deal with class imbalance, as tokens tagged to pickup point names are a minority.</p><p></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1526/1*WO1LRHQPqAqzpLBMMl-NuA.png\" class=\"kg-image\"></figure><p>With this, CartoBERT is fine-tuned to extract pickup point names from bookings text sentences, if any.</p><h2 id=\"how-does-the-model-perform\">How does the model perform?</h2><p>CartoBERT gives a lift of more than 25% in pickup point name accuracy to ~93% accuracy, which is measured as the percentage of valid pick up point names out of generated names. With this high accuracy, we have achieved full scalability of automatic generation for named pickup points to quickly cover multiple geographies without heavy reliance on human inputs.</p><h1 id=\"what-s-next\">What’s next?</h1><p>We are not stopping here and are exploring using active learning to further improve CartoBERT. With active learning, we only flag out uncertain predictions, which are measured as sentence level least token probability[3] for human labelling. We then use human-curated data as feedback for model learning. In this way, we can improve model learning efficiency with minimum labelling effort.</p><p>What’s more, with the success of CartoBERT, we are considering pre-training and open sourcing a general Indonesia Bahasa ALBERT model with Indonesia open corpus from wiki, news, Twitter etc. Currently, the options for open-sourced language model in Indonesia Bahasa are very limited, only pre-trained static word embeddings such as word2vec, fasttext etc are available. It would be beneficial to the community if we have a good state-of-the-art attention-based transformer model for the language. Stay tuned for more updates from the Cartography Data Science team. ?</p><p>Leave a ? if you liked what you read. Ping me with suggestions and feedback.</p><p>Thanks to all the amazing people who contributed to this post: Tan Funan, Zane Lim, Dang Le, Lijuan Chia, Bani Widyatmiko, Maureen Koha, Ringga Saputra, Nur Izzahudinr, Sandya Ardi, Yeni Primasari, Ardya Dipta.</p><hr><p><strong>References</strong></p><p>[1] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Devlin%2C+J\" rel=\"noopener\">J. Devlin</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chang%2C+M\" rel=\"noopener\">M. Chang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+K\" rel=\"noopener\">K. Lee</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Toutanova%2C+K\" rel=\"noopener\">K. Toutanova</a>: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.<br><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noopener\">arXiv:1810.04805</a> (2018)</p><p>[2] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lan%2C+Z\" rel=\"noopener\">Z. Lan</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M\" rel=\"noopener\">M. Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Goodman%2C+S\" rel=\"noopener\">S. Goodman</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gimpel%2C+K\" rel=\"noopener\">K. Gimpel</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Sharma%2C+P\" rel=\"noopener\">P. Sharma</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Soricut%2C+R\" rel=\"noopener\">R. Soricut</a>: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"noopener\">arXiv:1909.11942</a> (2019)</p><p>[3] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+M\" rel=\"noopener\">M.Liu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Tu%2C+Z\" rel=\"noopener\">Z. Tu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Z\" rel=\"noopener\">Z. Wang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+X\" rel=\"noopener\">X. Xu</a>: LTP: A New Active Learning Strategy for Bert-CRF Based Named Entity Recognition. <a href=\"https://arxiv.org/abs/2001.02524\" rel=\"noopener\">arXiv:2001.02524</a> (2020)</p><hr><p>Liked what you read? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">Sign up for our newsletter</a> to have our latest stories delivered straight to your inbox!</p>","url":"https://gojek-ghost.zysk.in/how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2/","canonical_url":null,"uuid":"51dd4492-b728-4b37-8c7f-41fd41619f7a","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb0f8af5524cd001e7391f4","reading_time":6}},{"node":{"id":"Ghost__Post__5eb12ec4f7c7da001effce0f","title":"The Secret Sauce Behind Search Personalisation","slug":"the-secret-sauce-behind-search-personalisation","featured":false,"feature_image":"https://res-5.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_x4G3c6-g3z3mz01o2EJdbw.jpg","excerpt":"How Gojek uses machine learning to personalise search results in GoFood.","custom_excerpt":"How Gojek uses machine learning to personalise search results in GoFood.","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"02 December, 2019","updated_at_pretty":"18 May, 2020","created_at":"2020-05-05T14:45:48.000+05:30","published_at":"2019-12-02T09:30:00.000+05:30","updated_at":"2020-05-18T20:43:26.000+05:30","meta_title":null,"meta_description":"How Gojek uses machine learning to personalise search results in GoFood.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"By Jewel James\n\nAre you looking for food? Thinking about burgers and crispy chicken fritters\nwith creamy sauces sitting on a warm, soft bun? At least, that’s what I am\nthinking about. May be you’re different. Vegan perhaps? Or craving noodles? Each\nof us turn into a different person when we are hungry, and this is the story\nabout how GoFood — Gojek’s food delivery service — helps you find the bite you\ncrave.\n\n> At the core of the food ordering experience, sits search.\nSearches are part of so many of our interactions on web and mobile, that we\ndon’t even notice it anymore. This is mostly because modern search engines have\nmatured to a point where they can deliver high quality results even if the\nentered query is a weak signal of the user’s intent. Even when weak, every query\nstill contains a piece of the user’s intent.\n\nIn the case of GoFood, that piece is a fragment of our user’s hunger-driven\nbrain.\n\nIn this post, we’ll discuss how we personalise the search results we surface on\nGoFood, based on the information we have about our users’ food preferences.\n\nHow Each of Us Differ\nLet’s look at two of our GoFood users who have started feeling the pangs of\nhunger, and come online to check restaurants near them.\n\nThey both open our app and click on the NEAR ME tile that lets users find the\nrestaurants near them. We show them the nearest restaurants first, and this is\nwhat they both see.\n\n\nThis list goes on and on and will let them see each restaurant which is farther\nand farther away. They can now peruse the menus of each restaurant and pick one\nthat serves what they feel like ordering. The problem is that they may spend\nsome time scrolling and then leave the app without being able to make up their\nminds around which restaurant to place an order from. We have thrown too many\nchoices at them and the cognitive effort of picking a restaurant and then a dish\nfrom the menu is too much work to make them suffer through.\n\nBut wait. Both Mila and Husain have transacted with us in the past, and we know\na fair bit about their preferences. 🤔\n\nTo make this experience better, we decided to build a system that would let both\nof them see restaurants that suit their own tastes and preferences\n\nApplying Machine Learning to the Problem\nRanking documents for relevance works by assigning a prediction score to each\ndocument retrieved, which is directly proportional to its relevance. In the case\nof NEAR ME restaurant ranking this can be something like:\n\n> Relevance score = 2 * (1/distance) + 1.2 * rating of restaurant\nHere in the relevance score calculation, we are taking weighted sum of different\nfactors like (1/distance) and rating of restaurant. The coefficients/weights of\nthese factors can be arrived at by experimenting with them and choosing weights\nthat seem to maximise the ordering conversions. But, in the case of restaurant\nranking in GoFood, we want to take into consideration many factors when deciding\nrelevance. Unfortunately, experimenting with combinations of all those factors\nis impossible.\n\nEnter Learning to rank. Here, the problem of deciding the rank of the\nrestaurants shown to the user is formulated as a supervised machine learning\nproblem.\n\nIf we look at past search, click, and ordering data, we will be able to assign\nrelevance judgements to each restaurant listing according to whether our users\nclicked or ordered from one of those restaurants. Restaurants that attracted\nhigher degrees of interest will be given higher degrees of relevance.\n\nIn the below example, a relevance judgement level of 0,1 and 2 is assigned to a\nrestaurant according to whether the user viewed, clicked, or ordered from the\nrestaurant in the search result. The relevance judgements are relative and only\nintent to be monotonically increasing with increasing relevance . They say that\nthe restaurant which the user created an order from is more relevant than a\nrestaurant the user merely checked out by clicking on it. They don’t mean that\nthe restaurant which received the order is twice as relevant .\n\nThe values of each of the factors that could have played a role in the user’s\ndecision of clicking or ordering is also shown against the restaurants.\n\nThree of the factors/features in the above example are marked as\n‘personalisation features’ because they would change according to the user’s\nprevious order history and location. These will be the features that will be\ndifferent between Mila and Husain because of the differences in the restaurants\nand cuisines they have ordered from before in the past.\n\n> These personalisation features are at the crux of creating personalised\nexperiences for each user\nOther customer agnostic features/factors like the rating, price range, and\npopularity of the restaurant are also listed here. GoFood has millions of such\nexamples where users with different tastes made different decisions when shown a\nset of search results. These examples can now be used to create a dataset from\nwhich the learning to rank ML algorithm can create a model to decide how\nrelevant a GoFood user would find a restaurant given that user’s location, order\nhistory and other restaurant statistics.\n\nHow We Ranked\nOne way to approach this was as a point-wise ranking problem, wherein we try to\npredict the relevance judgement of each restaurant. Based on this, later we will\npredict the relevance judgement level and sort restaurants in decreasing order\nof predicted relevance score. This approach reduces learning to rank problem to\na regression problem.\n\nAnother approach was to solve it is a pairwise ranking problem, wherein the ML\nmodel is trying to learn how to get the order of a pair of restaurants correct\ni.e if Restaurant A is more preferable to Restaurant B , the order (Restaurant\nA, Restaurant B) is correct and the order (Restaurant B, Restaurant A) is wrong.\n\nFor an ML model to be learned, we need an objective function that captures this\npairwise ordering formulation . This is called a loss function or error function\nin ML and is the measure through which an ML model can assess how right or wrong\nits decision was. In pairwise ranking , this should be a function that becomes\nhigher whenever the model misjudges a preference order and becomes lower when it\nis right about the preference order.\n\nThe loss/error function C is explained below:\n\nWhen this function is minimised, the model is trying to predict a score for each\nrestaurant such that the ranking of the order of the restaurants are close the\nrelevance judgements the users made.\n\nThe pairwise formulation is a better approach here in comparison to the\npoint-wise approach as it is looking to get the order of restaurants right and\nis not trying to estimate the relevance score themselves whose values were\nassigned only as markers to show how some restaurants were more preferred\nrelative to others.\n\nWe used an implementation of the LambdaMART algorithm that learns to predict\nrelevance scores so as to minimise this pairwise loss. You can think of this as\na pursuit to find the decision tree that takes in all the parameters of the\nrestaurant and gives out a score to the restaurant . This score should be\nassigned in such a way as to make the pair orders right.\n\nOnce this model is trained, it can be used during search, as explained in this\npost\n[https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a?source=friends_link&sk=42397976fe914a418ac40f19545f90b7]\n.\n\nNow let’s go back to our beloved customers — Mila and Husain. The next time Mila\nor Husain looks for restaurants near them, the search results they see will be\naccording to their preferences. This is because the model would look at the\nnumber of times they have ordered from each of the restaurants near them before.\nIt would take their preferred cuisines and factors like restaurant ratings into\naccount, and show them the restaurants that they would prefer to order from\nfirst.\n\nThe different search results Mila and Husain get after learning to rank is used\nto re-rank the results are shown below.\n\nWe ran an AB test using this formulation of learning to rank and observed a\nrelative lift of 20% in search to ordering conversions and 23% improvement in\nNDCG. More information on how this metric is calculated here\n[https://blog.gojekengineering.com/is-this-what-you-were-looking-for-439bf012cca6?source=friends_link&sk=bdc1310acc3b6a8270f10284cb30fa53]\n.\n\nWhat We Learned\nOne interesting thing we observed as we started experimenting with this learning\nto rank model was personalising search results led to the average position at\nwhich the search to order conversions happen to be much higher on the list. This\nis because users were increasingly seeing the restaurants that they have some\naffinity towards and were able to make an ordering decision without scrolling\nmuch and without spending too much time being confused where to order from.\n\nSo that’s how we rank restaurant pairs with respect to relevance based on\navailable user data. We’ll continue to write more about how we make our products\nmore intuitive. Stay tuned to this blog, or sign up for our newsletter\n[https://mailchi.mp/go-jek/gojek-tech-newsletter] for email updates. 👌\n\n\n--------------------------------------------------------------------------------\n\n(Special thanks to Sugam Anand [https://twitter.com/SugamAnand] for additional\ndesign support ✌️)\n\n\n--------------------------------------------------------------------------------\n\nWant weekly updates with more of our stories? Sign up for our newsletter!\n[https://mailchi.mp/go-jek/gojek-tech-newsletter] ✌️","html":"<p>By Jewel James</p><p>Are you looking for food? Thinking about burgers and crispy chicken fritters with creamy sauces sitting on a warm, soft bun? At least, that’s what I am thinking about. May be you’re different. Vegan perhaps? Or craving noodles? Each of us turn into a different person when we are hungry, and this is the story about how GoFood — Gojek’s food delivery service — helps you find the bite you crave.</p><blockquote><em><em>At the core of the food ordering experience, sits search.</em></em></blockquote><p>Searches are part of so many of our interactions on web and mobile, that we don’t even notice it anymore. This is mostly because modern search engines have matured to a point where they can deliver high quality results even if the entered query is a weak signal of the user’s intent. Even when weak, every query still contains a piece of the user’s intent.</p><p><em><em>In the case of GoFood, that piece is a fragment of our user’s hunger-driven brain.</em></em></p><p>In this post, we’ll discuss how we personalise the search results we surface on GoFood, based on the information we have about our users’ food preferences.</p><h1 id=\"how-each-of-us-differ\">How Each of Us Differ</h1><p>Let’s look at two of our GoFood users who have started feeling the pangs of hunger, and come online to check restaurants near them.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/3192/1*AP6fdakISbNOdEZyCUxa5g.jpeg\" class=\"kg-image\"></figure><p>They both open our app and click on the <code>NEAR ME</code> tile that lets users find the restaurants near them. We show them the nearest restaurants first, and this is what they both see.<br></p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/2322/1*tBxtTKY9cfA7rcl5LXq3Ag.jpeg\" class=\"kg-image\"></figure><p>This list goes on and on and will let them see each restaurant which is farther and farther away. They can now peruse the menus of each restaurant and pick one that serves what they feel like ordering. The problem is that they may spend some time scrolling and then leave the app without being able to make up their minds around which restaurant to place an order from. We have thrown too many choices at them and the cognitive effort of picking a restaurant and then a dish from the menu is too much work to make them suffer through.</p><p><strong><strong><em><em>But wait. Both Mila and Husain have transacted with us in the past, and we know a fair bit about their preferences.</em></em> 🤔</strong></strong></p><p>To make this experience better, we decided to build a system that would let both of them see restaurants that suit their own tastes and preferences</p><h1 id=\"applying-machine-learning-to-the-problem\">Applying Machine Learning to the Problem</h1><p>Ranking documents for relevance works by assigning a prediction score to each document retrieved, which is directly proportional to its relevance. In the case of <code>NEAR ME</code> restaurant ranking this can be something like:</p><blockquote><em><em><em>Relevance score = 2 * (1/distance) + 1.2 * rating of restaurant</em></em></em></blockquote><p>Here in the relevance score calculation, we are taking weighted sum of different factors like <em><em>(1/distance) </em></em>and <em><em>rating of restaurant</em></em>. The coefficients/weights of these factors can be arrived at by experimenting with them and choosing weights that seem to maximise the ordering conversions. But, in the case of restaurant ranking in GoFood, we want to take into consideration many factors when deciding relevance. Unfortunately, experimenting with combinations of all those factors is impossible.</p><p>Enter L<em><em>earning to rank</em></em>. Here, the problem of deciding the rank of the restaurants shown to the user is formulated as a supervised machine learning problem.</p><p>If we look at past search, click, and ordering data, we will be able to assign relevance judgements to each restaurant listing according to whether our users clicked or ordered from one of those restaurants. Restaurants that attracted higher degrees of interest will be given higher degrees of relevance.</p><p>In the below example, a relevance judgement level of 0,1 and 2 is assigned to a restaurant according to whether the user viewed, clicked, or ordered from the restaurant in the search result. The relevance judgements are relative and only intent to be monotonically increasing with increasing relevance . They say that the restaurant which the user created an order from is more relevant than a restaurant the user merely checked out by clicking on it. <strong><strong>They don’t mean that the restaurant which received the order is twice as relevant .</strong></strong></p><p>The values of each of the factors that could have played a role in the user’s decision of clicking or ordering is also shown against the restaurants.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/2376/1*X0Oe0KA7ZTU2wYa2SXS_CA.png\" class=\"kg-image\"></figure><p>Three of the factors/features in the above example are marked as ‘personalisation features’ because they would change according to the user’s previous order history and location. These will be the features that will be different between Mila and Husain because of the differences in the restaurants and cuisines they have ordered from before in the past.</p><blockquote>These personalisation features are at the crux of creating personalised experiences for each user</blockquote><p>Other customer agnostic features/factors like the rating, price range, and popularity of the restaurant are also listed here. GoFood has millions of such examples where users with different tastes made different decisions when shown a set of search results. These examples can now be used to create a dataset from which the learning to rank ML algorithm can create a model to decide how relevant a GoFood user would find a restaurant given that user’s location, order history and other restaurant statistics.</p><h1 id=\"how-we-ranked\">How We Ranked</h1><p>One way to approach this was as a point-wise ranking problem, wherein we try to predict the relevance judgement of each restaurant. Based on this, later we will predict the relevance judgement level and sort restaurants in decreasing order of predicted relevance score. This approach reduces learning to rank problem to a regression problem.</p><p>Another approach was to solve it is a pairwise ranking problem, wherein the ML model is trying to learn how to get the order of a pair of restaurants correct i.e if Restaurant A is more preferable to Restaurant B , the order (Restaurant A, Restaurant B) is correct and the order (Restaurant B, Restaurant A) is wrong.</p><p>For an ML model to be learned, we need an objective function that captures this pairwise ordering formulation . This is called a loss function or error function in ML and is the measure through which an ML model can assess how right or wrong its decision was. In pairwise ranking , this should be a function that becomes higher whenever the model misjudges a preference order and becomes lower when it is right about the preference order.</p><p>The loss/error function <strong><strong><em><em>C</em></em></strong></strong> is explained below:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/3316/1*2sWi_7l1iI_-dRppVoaGRA.png\" class=\"kg-image\"></figure><p>When this function is minimised, the model is trying to predict a score for each restaurant such that the ranking of the order of the restaurants are close the relevance judgements the users made.</p><p>The pairwise formulation is a better approach here in comparison to the point-wise approach as it is looking to get the order of restaurants right and is not trying to estimate the relevance score themselves whose values were assigned only as markers to show how some restaurants were more preferred relative to others.</p><p>We used an implementation of the LambdaMART algorithm that learns to predict relevance scores so as to minimise this pairwise loss. You can think of this as a pursuit to find the decision tree that takes in all the parameters of the restaurant and gives out a score to the restaurant . This score should be assigned in such a way as to make the pair orders right.</p><p>Once this model is trained, it can be used during search, as explained in <a href=\"https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a?source=friends_link&amp;sk=42397976fe914a418ac40f19545f90b7\">this post</a>.</p><p>Now let’s go back to our beloved customers — Mila and Husain. The next time Mila or Husain looks for restaurants near them, the search results they see will be according to their preferences. This is because the model would look at the number of times they have ordered from each of the restaurants near them before. It would take their preferred cuisines and factors like restaurant ratings into account, and show them the restaurants that they would prefer to order from first.</p><p>The different search results Mila and Husain get after learning to rank is used to re-rank the results are shown below.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/7545/1*ALMsK5OzG3rUACrx_U3E0g.jpeg\" class=\"kg-image\"></figure><p>We ran an AB test using this formulation of learning to rank and observed a relative lift of 20% in search to ordering conversions and 23% improvement in NDCG. More information on how this metric is calculated <a href=\"https://blog.gojekengineering.com/is-this-what-you-were-looking-for-439bf012cca6?source=friends_link&amp;sk=bdc1310acc3b6a8270f10284cb30fa53\" rel=\"noopener\">here</a>.</p><h1 id=\"what-we-learned\">What We Learned</h1><p>One interesting thing we observed as we started experimenting with this learning to rank model was personalising search results led to the average position at which the search to order conversions happen to be much higher on the list. This is because users were increasingly seeing the restaurants that they have some affinity towards and were able to make an ordering decision without scrolling much and without spending too much time being confused where to order from.</p><p>So that’s how we rank restaurant pairs with respect to relevance based on available user data. We’ll continue to write more about how we make our products more intuitive. Stay tuned to this blog, or <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">sign up for our newsletter</a> for email updates. 👌</p><hr><p>(Special thanks to <a href=\"https://twitter.com/SugamAnand\" rel=\"noopener\">Sugam Anand</a> for additional design support ✌️)</p><hr><p>Want weekly updates with more of our stories? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">Sign up for our newsletter!</a> ✌️</p>","url":"https://gojek-ghost.zysk.in/the-secret-sauce-behind-search-personalisation/","canonical_url":null,"uuid":"f53d2946-dab7-43e3-af3b-5453077e95ee","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb12ec4f7c7da001effce0f","reading_time":7}},{"node":{"id":"Ghost__Post__5ec2ca287aa22c4066f83b68","title":"Batch Processing Pipelines for Better Data Analysis","slug":"batch-processing-pipelines-for-better-data-analysis","featured":false,"feature_image":"https://gojek-ghost.zysk.in/content/images/2020/05/1_EbT2AH9uAMjxHrVxh5xJrw.jpeg","excerpt":"How we generate intelligible insights from our data warehouse using batch pipelines.","custom_excerpt":"How we generate intelligible insights from our data warehouse using batch pipelines.","visibility":"public","created_at_pretty":"18 May, 2020","published_at_pretty":"11 November, 2019","updated_at_pretty":"18 May, 2020","created_at":"2020-05-18T23:17:20.000+05:30","published_at":"2019-11-11T09:30:00.000+05:30","updated_at":"2020-05-18T23:34:54.000+05:30","meta_title":null,"meta_description":"How we generate intelligible insights from our data warehouse using batch pipelines.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"By Maulik Soneji\n\nGojek’s Data Warehouse [https://en.wikipedia.org/wiki/Data_warehouse], built by\nintegrating data from multiple applications and sources, serves as a central\npoint of analysis that also helps generate actionable insights. Our batch\npipelines process billions of data points periodically, in order to help our\nbusiness teams gather an effective view of data.\n\nThis post explains our approach to building batch pipelines that leverage\ncomplex data in an efficient way.\n\nI will start by providing some context on our data warehouse and the data we\nstore in it, and explain the use cases of batch processing that we tackle at\nGojek. Then we’ll talk about how we tackle running batch processing jobs and\nhandling scheduling dependencies.\n\nData Warehouse setup\nAt Gojek, we use Google Bigquery (BQ) as a Data Warehouse. All data points\nranging from booking and searches to location are published in real time using \nBeast [https://github.com/gojek/beast], our open-sourced tool.\n\nWe also push data from Kafka to our data lake, which is Google Cloud\nStorage(GCS). These data points vary in terms of the data format (which might\nnot be relevant to the analysts). They need to gather insights from the data\nwithout needing to know how it is stored.\n\n> To solve this, we wanted to create an abstraction such that the users of data\nonly need to know about the constitution of data, and not about where the data\nis coming from or what format the data is stored.\nBatch processing use cases\nTypical use cases of batch processing at Gojek revolve around enriching\nreal-time data with additional data points mined from huge amounts of historical\ndata.\n\nA few examples of use cases include:\n\nCreating a customer profile:\n\nIn order to provide our customers with the most relevant discount and deal\nvouchers, we enrich customer profiles with the last few months of the customer’s\norder and search history. This enables our team of data analysts and data\nscientists to experiment with customer segmentation and targeting. This use case\nhas been covered in much detail by my colleague Mayank in this blog\n[https://blog.gojekengineering.com/how-we-solved-user-selection-to-help-merchants-win-business-519fe5085a0e]\n.\n\nPersonalising search results:\n\nIn order to personalise the search results served up by our food delivery app\nGoFood, we leverage batch processing to gather insights about trending, popular,\nand highly-rated restaurants near the user that match their taste profile. More\ndetails around how we went about this use case are covered in this blog\n[https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a]\n.\n\nRunning Batch pipelines\nAs I previously mentioned, the users of data usually don’t need to know the\nformat in which the data is stored. They would benefit from having a unified\ninterface to interact with data.\n\nWe leveraged Dataframes in Apache Spark [https://spark.apache.org/] to provide\nthe unified interface.\n\nDataFrames or Datasets\nSpark provides an abstraction on top of the data underneath — called DataFrames\nor Datasets.\n\nDataFrames are distributed collections of data in which data is organised in the\nform of columns.\n\nConceptually, a data frame becomes similar to a database.\n\nFew examples of reading from Bigquery and GCS are as follows:\n\nClient to read data from bigquery into spark dataframe.\n\nClient to read data from GCS into spark dataframe\n\nUsing these clients make it very easy for our analysts to read GCS and Bigquery\ndata into Spark and interact with it.\n\nRunning Spark Jobs\nWe use Google Dataproc [https://cloud.google.com/dataproc/] hosting a Spark\ncluster to run our batch pipelines. On each trigger of a batch job, we create an\nephemeral cluster to run the job, which means that the cluster is destroyed\nafter the batch job completes.\n\nThe batch job is written in Pyspark\n[https://spark.apache.org/docs/2.2.0/api/python/pyspark.html], which all our\nanalysts are familiar with. This provides a good interface to interact with\nSpark Dataframes.\n\nScheduling Dependencies between Jobs\nAs the Spark jobs become more complex and handle many responsibilities, it\nbecomes important to break them down into simpler jobs that can be better\nmanaged.\n\n> But this breakdown brings more challenges.\nWe now have to make sure the related jobs are scheduled taking in mind the\nscheduling dependencies between different jobs.\n\nFor example:\nIf there are two jobs, the first one calculates the last 6 months of order\nhistory and the second job uses the order history to calculate the preferred\nlocations from which the customer has ordered, it becomes important to run the\nfirst job and then schedule the second job.\n\nOur solution to handle such scheduling dependencies is to use Apache Airflow\n[https://airflow.apache.org/]. This is a tool to programmatically schedule and\nmanage scheduling dependencies between different jobs.\n\nThe scheduling dependencies are written as a Directed Acyclic Graph (DAG) and we\nset a schedule for the DAG to run. Simple. 🙂\n\nWith Airflow, we are also able to assign retries for each job. In the instance\nof a job failing, Airflow will rerun the job by itself.\n\nAs a final precaution, we have also added Slack integration and StatsD metrics\nwith Airflow, in order to get alerts for when the jobs have failed and need to\nbe fixed.\n\nSo that’s all for this post. Hope you liked it! If you’d like to work on cool\nproblems and help us scale a #SuperApp for Southeast Asia, make sure to check\nout gojek.jobs [http://bit.ly/2CvjmXv]. Until next time. 🖖\n\n\n--------------------------------------------------------------------------------\n\nWant our updates delivered straight to your inbox? Sign up for our newsletter!\n[https://mailchi.mp/go-jek/gojek-tech-newsletter]","html":"<p>By Maulik Soneji</p><p>Gojek’s <a href=\"https://en.wikipedia.org/wiki/Data_warehouse\" rel=\"noopener\">Data Warehouse</a>, built by integrating data from multiple applications and sources, serves as a central point of analysis that also helps generate actionable insights. Our batch pipelines process billions of data points periodically, in order to help our business teams gather an effective view of data.</p><p>This post explains our approach to building batch pipelines that leverage complex data in an efficient way.</p><p>I will start by providing some context on our data warehouse and the data we store in it, and explain the use cases of batch processing that we tackle at Gojek. Then we’ll talk about how we tackle running batch processing jobs and handling scheduling dependencies.</p><h1 id=\"data-warehouse-setup\">Data Warehouse setup</h1><p>At Gojek, we use Google Bigquery (BQ) as a Data Warehouse. All data points ranging from booking and searches to location are published in real time using <a href=\"https://github.com/gojek/beast\" rel=\"noopener\"><strong><strong>Beast</strong></strong></a><strong><strong>, </strong></strong>our open-sourced tool.</p><p>We also push data from Kafka to our data lake, which is Google Cloud Storage(GCS). These data points vary in terms of the data format (which might not be relevant to the analysts). They need to gather insights from the data without needing to know how it is stored.</p><blockquote><em><em>To solve this, we wanted to create an abstraction such that the users of data only need to know about the constitution of data, and not about where the data is coming from or what format the data is stored.</em></em></blockquote><h1 id=\"batch-processing-use-cases\"><strong>Batch processing use cases</strong></h1><p>Typical use cases of batch processing at Gojek revolve around enriching real-time data with additional data points mined from huge amounts of historical data.</p><p>A few examples of use cases include:</p><p><strong><strong>Creating a customer profile:</strong></strong></p><p>In order to provide our customers with the most relevant discount and deal vouchers, we enrich customer profiles with the last few months of the customer’s order and search history. This enables our team of data analysts and data scientists to experiment with customer segmentation and targeting. This use case has been covered in much detail by my colleague Mayank in this <a href=\"https://blog.gojekengineering.com/how-we-solved-user-selection-to-help-merchants-win-business-519fe5085a0e\" rel=\"noopener\">blog</a>.</p><p><strong><strong>Personalising search results:</strong></strong></p><p>In order to personalise the search results served up by our food delivery app GoFood, we leverage batch processing to gather insights about trending, popular, and highly-rated restaurants near the user that match their taste profile. More details around how we went about this use case are covered in this <a href=\"https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a\" rel=\"noopener\">blog</a>.</p><h1 id=\"running-batch-pipelines\">Running Batch pipelines</h1><p>As I previously mentioned, the users of data usually don’t need to know the format in which the data is stored. They would benefit from having a unified interface to interact with data.</p><p><em><em>We leveraged Dataframes in </em></em><a href=\"https://spark.apache.org/\" rel=\"noopener\"><em><em>Apache Spark</em></em></a><em><em> to provide the unified interface.</em></em></p><h2 id=\"dataframes-or-datasets\"><strong>DataFrames or Datasets</strong></h2><p>Spark provides an abstraction on top of the data underneath — called DataFrames or Datasets.</p><p>DataFrames are distributed collections of data in which data is organised in the form of columns.</p><p><strong><strong>Conceptually, a data frame becomes similar to a database.</strong></strong></p><p>Few examples of reading from Bigquery and GCS are as follows:</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/mauliksoneji/48d7d84976ee5957de90e03ba2314540.js\"></script><!--kg-card-end: html--><p><em>Client to read data from bigquery into spark dataframe.</em></p><!--kg-card-begin: html--><script src=\"https://gist.github.com/mauliksoneji/0a8c12d3c7ecbe2c4794dbd039e03815.js\"></script><!--kg-card-end: html--><p><em>Client to read data from GCS into spark dataframe</em></p><p>Using these clients make it very easy for our analysts to read GCS and Bigquery data into Spark and interact with it.</p><p><strong><strong>Running Spark Jobs</strong></strong><br>We use <a href=\"https://cloud.google.com/dataproc/\" rel=\"noopener\">Google Dataproc</a> hosting a Spark cluster to run our batch pipelines. On each trigger of a batch job, we create an ephemeral cluster to run the job, which means that the cluster is destroyed after the batch job completes.</p><p>The batch job is written in <a href=\"https://spark.apache.org/docs/2.2.0/api/python/pyspark.html\" rel=\"noopener\">Pyspark</a>, which all our analysts are familiar with. This provides a good interface to interact with Spark Dataframes.</p><h1 id=\"scheduling-dependencies-between-jobs\"><strong>Scheduling Dependencies between Jobs</strong></h1><p>As the Spark jobs become more complex and handle many responsibilities, it becomes important to break them down into simpler jobs that can be better managed.</p><blockquote>But this breakdown brings more challenges.</blockquote><p>We now have to make sure the related jobs are scheduled taking in mind the scheduling dependencies between different jobs.</p><p><strong><strong>For example:</strong></strong><br>If there are two jobs, the first one calculates the last 6 months of order history and the second job uses the order history to calculate the preferred locations from which the customer has ordered, it becomes important to run the first job and then schedule the second job.</p><p>Our solution to handle such scheduling dependencies is to use <a href=\"https://airflow.apache.org/\" rel=\"noopener\">Apache Airflow</a>. This is a tool to programmatically schedule and manage scheduling dependencies between different jobs.</p><p>The scheduling dependencies are written as a Directed Acyclic Graph (DAG) and we set a schedule for the DAG to run. Simple. 🙂</p><p>With Airflow, we are also able to assign retries for each job. In the instance of a job failing, Airflow will rerun the job by itself.</p><p>As a final precaution, we have also added Slack integration and StatsD metrics with Airflow, in order to get alerts for when the jobs have failed and need to be fixed.</p><p>So that’s all for this post. Hope you liked it! If you’d like to work on cool problems and help us scale a #SuperApp for Southeast Asia, make sure to check out <a href=\"http://bit.ly/2CvjmXv\" rel=\"noopener\">gojek.jobs</a>. Until next time. 🖖</p><hr><p>Want our updates delivered straight to your inbox? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\">Sign up for our newsletter!</a></p>","url":"https://gojek-ghost.zysk.in/batch-processing-pipelines-for-better-data-analysis/","canonical_url":null,"uuid":"0d510c4e-75d7-409e-97c0-5414539cb91f","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ec2ca287aa22c4066f83b68","reading_time":3}},{"node":{"id":"Ghost__Post__5eb0fef85524cd001e739246","title":"Efficient Experimentation at Gojek","slug":"beast-moving-data-from-kafka-to-bigquery","featured":false,"feature_image":"https://res-5.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_VVhMvfqoGY_h6W0-VW4iMQ.jpg","excerpt":"A breakdown of the tool we use to estimate the right audience sample size for our experiments.","custom_excerpt":"A breakdown of the tool we use to estimate the right audience sample size for our experiments.","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"02 September, 2019","updated_at_pretty":"12 May, 2020","created_at":"2020-05-05T11:21:52.000+05:30","published_at":"2019-09-02T09:30:00.000+05:30","updated_at":"2020-05-12T11:53:43.000+05:30","meta_title":"Efficient Experimentation at Gojek","meta_description":"A breakdown of the tool we use to estimate the right audience sample size for our experiments.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Experiments are wonderful things. They help us validate our hypotheses without\nhaving to involve our entire user base. However, any good experiment first\nrequires the answer to the question — how do we know the right number of people\nto include in it?\n\nLet’s break down why this is important. If we target too few, our experiments\nwill not detect smaller effects, leading us to miss legitimate opportunities. If\nwe target too many, we would be wasting resources and potentially hurting our\nbusiness metrics (if the treatments turn out to be detrimental).\n\nPreviously, we used simple heuristics (and a mysterious formula) to estimate the\nrequired sample size. These practices were not very scientific and produced\ninconsistent experimental results. To compensate, we had to repeat the same\nexperiment multiple times just to validate the results. Sometimes different\niterations would conflict with each other, leaving everyone scratching their\nheads.\n\nLast year, we ditched those unscientific practices and built a new tool called\nSample Size Calculator (yes, imaginative name, we know). This post explains how\nthe calculator helps us find the right number of people to include in\nexperiments.\n\nOur calculator is based on Frequentist school of statistics. Under the hood, we\nused both the pwr\n[https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html] \npackage and the base sample size calculation functions in R programming\nlanguage. This is what the tool looks like:\n\nFigure 1. Screenshot of the Sample Size Calculator used at GojekThe most\nimportant parameters are the type of dependent variable measured, the historical\ndata, and the required sensitivity from the experiment. As you can see in the\ngraph, if we increase sensitivity to allow detection of smaller effects, the\nrequired sample size soars, which makes sense.\n\nAlthough this is a major step in the right direction, there is a problem if we\nare interested in a continuous dependent variable for our experiment. This is\nbecause the underlying methodology used in the calculator assumes a normal\ndistribution, while most continuous metrics that we care about at Gojek — such\nas bookings per user — are usually skewed. By ‘skewed’, we mean that most users\nare light users and only a small percentage are heavy users. This could cause\nthe computed sample size requirement to be wrong because a key assumption was\nnot met.\n\nWe have four options to deal with this problem:\n\n 1. Use a methodology that doesn’t assume normality. Unfortunately, the\n    available non-parametric methods are complex to implement [1\n    [http://www.biostat.jhsph.edu/~ejohnson/regression/Sample%20Size%20Power%20Considerations.pdf]\n    ] [2 [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4423589/]]\n 2. Transform data to have a normal distribution. This doesn’t always work —\n    some data just can’t be coerced to normal\n 3. Use a larger sample size. Since the worry is lower power than expected,\n    simple multiplication should boost the power, but the multiplier needs to be\n    reasonable. Figuring the right multiplier would be another beast on its own\n 4. Ignore it. This is the path we took\n\n> But but but… that’s not scientific!\nDon’t worry. We tested the robustness of this solution before going ahead with\nit.\n\nAs you can see in the graph below, we created two right-skewed distributions\nwith known 5% difference in means to serve as our populations. The plan is to\ncontinuously sample from this population (equivalent to replicating the same\nexperiment many times) and see how often we can detect this 5% difference (aka\nwhat is our “power [https://en.wikipedia.org/wiki/Power_(statistics)]”).\n\nFigure 2. Two right-skewed distributions (typical shape for continuous metric at\nGojek) with known mean difference (lift) of 5%The first step is to calculate the\nrequired sample size with a power of 80% and minimum detectable effect of 5%.\nLet’s refer to this sample size requirement as the “exact” size. Then we divide\nand multiply it by three and call them “less” and “more”, respectively. For each\nsample size, we sample from the populations, compute the mean difference\n(typically called “lift”), and repeat this procedure many times.\n\nThe resulting distributions of lifts are shown below. The coloured vertical\nlines show the average lift of each experimental size, which is also equal to\nthe actual population lift. This is central limit theorem\n[https://en.wikipedia.org/wiki/Central_limit_theorem] at work. What is\ninteresting are the relative shapes of the sampling distributions. The “more”\ngroup is the safest and most consistent but the “exact” group seems to have good\naccuracy with only a third of the sample size. In the “less” group, we even see\na non-negligible number of lifts that are negative, even though we know the\npopulation lift is actually +5%. This is in line with our intuition that larger\nsample size is generally more trustworthy than a smaller one, all else being\nequal.\n\nFigure 3. Sampling distribution of the mean difference from each size groupSince\nwe currently report the statistical significance of our experimental results at\nGojek, we will also run t-test for each of our sample here.\n\nAs it turns out, 80.4% of the experiments in the “exact” group have\nstatistically significant results (p < 0.05), which is almost equal to the 80%\npower we set in the sample size calculator. In layman’s terms, the tool roughly\npromised that if you run the experiment with this sample size, you have 80%\nchance of detecting a 5% lift (the minimum detectable effect) if it’s there.\n\nSince it kept its promise even on non-normally distributed data, we can conclude\nthat the methodology is robust to some degree of violation of the normality\nassumption. Therefore we can ignore it until otherwise proven.\n\nFigure 4. Percentage of samples/experiments with statistically significant\nresults (p < 0.05) from each size groupMeanwhile, the “less” group had only ~34%\nstatistically significant results, suggesting that we shouldn’t run an\nexperiment if we cannot fulfil the sample size requirement — since we risk not\nbeing able to detect an existing effect. With the “more” group having close to\n100% statistically significant results, we again reaffirm that larger sample\nsize is generally a good thing if we can afford it.\n\nSince it was deployed in mid-2018, many teams across Gojek have adopted the\nSample Size Calculator. So far, nearly 1,000 experiments have incorporated this\ntool in their designs, and it is also a part of Litmus, our experimentation\nplatform\n[https://blog.gojekengineering.com/introducing-litmus-gojeks-own-experimentation-platform-3803467b6a53]\n. The more critical takeaway for us is that the experiments run using the\nrecommended sample sizes have similar results when full-scaled. This fact has\nenabled us to iterate faster and cut waste while maintaining a high degree of\nconfidence in the experimental results. ✌️\n\nDid we mention Gojek’s Growth team is hiring analysts? We are a super\ndata-driven team and have helped shape some of the company’s best practices\nthrough projects like this. If you are analytical, interested in sharpening your\ntechnical chops, and want to have a meaningful impact to Gojek’s hypergrowth,\ncome join us!\n\n\n--------------------------------------------------------------------------------\n\nFor more updates like this, don’t forget to sign up for our newsletter\n[https://mailchi.mp/go-jek/gojek-tech-newsletter]!","html":"<p>Experiments are wonderful things. They help us validate our hypotheses without having to involve our entire user base. However, any good experiment first requires the answer to the question — how do we know the right number of people to include in it?</p><p>Let’s break down why this is important. If we target too few, our experiments will not detect smaller effects, leading us to miss legitimate opportunities. If we target too many, we would be wasting resources and potentially hurting our business metrics (if the treatments turn out to be detrimental).</p><p>Previously, we used simple heuristics (and a mysterious formula) to estimate the required sample size. These practices were not very scientific and produced inconsistent experimental results. To compensate, we had to repeat the same experiment multiple times just to validate the results. Sometimes different iterations would conflict with each other, leaving everyone scratching their heads.</p><p>Last year, we ditched those unscientific practices and built a new tool called Sample Size Calculator (yes, imaginative name, we know). This post explains how the calculator helps us find the right number of people to include in experiments.</p><p>Our calculator is based on Frequentist school of statistics. Under the hood, we used both the <a href=\"https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html\" rel=\"noopener\"><em><em>pwr</em></em></a> package and the base sample size calculation functions in R programming language. This is what the tool looks like:</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://miro.medium.com/max/1600/0*4kcE2olMNjGN4WFC\" class=\"kg-image\"><figcaption><strong>Figure 1.</strong> Screenshot of the Sample Size Calculator used at Gojek</figcaption></figure><p>The most important parameters are the type of dependent variable measured, the historical data, and the required sensitivity from the experiment. As you can see in the graph, if we increase sensitivity to allow detection of smaller effects, the required sample size soars, which makes sense.</p><p>Although this is a major step in the right direction, there is a problem if we are interested in a continuous dependent variable for our experiment. This is because the underlying methodology used in the calculator assumes a normal distribution, while most continuous metrics that we care about at Gojek — such as <code>bookings per user</code> — are usually skewed. By ‘skewed’, we mean that most users are light users and only a small percentage are heavy users. This could cause the computed sample size requirement to be wrong because a key assumption was not met.</p><p>We have four options to deal with this problem:</p><ol><li>Use a methodology that doesn’t assume normality. Unfortunately, the available non-parametric methods are complex to implement [<a href=\"http://www.biostat.jhsph.edu/~ejohnson/regression/Sample%20Size%20Power%20Considerations.pdf\" rel=\"noopener\">1</a>] [<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4423589/\" rel=\"noopener\">2</a>]</li><li>Transform data to have a normal distribution. This doesn’t always work — some data just can’t be coerced to normal</li><li>Use a larger sample size. Since the worry is lower power than expected, simple multiplication should boost the power, but the multiplier needs to be reasonable. Figuring the right multiplier would be another beast on its own</li><li>Ignore it. <strong><strong>This is the path we took</strong></strong></li></ol><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/350/1*lDXre2rWb6KOp8zOdx7V-Q.gif\" class=\"kg-image\"></figure><blockquote><strong>But but but… that’s not scientific!</strong></blockquote><p><strong><strong>Don’t worry. We tested the robustness of this solution before going ahead with it.</strong></strong></p><p>As you can see in the graph below, we created two right-skewed distributions with known 5% difference in means to serve as our populations. The plan is to continuously sample from this population (equivalent to replicating the same experiment many times) and see how often we can detect this 5% difference (aka what is our “<a href=\"https://en.wikipedia.org/wiki/Power_(statistics)\" rel=\"noopener\">power</a>”).</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://miro.medium.com/max/1120/0*vJpyQEMT_z3AgDSs\" class=\"kg-image\"><figcaption><strong>Figure 2.</strong> Two right-skewed distributions (typical shape for continuous metric at Gojek) with known mean difference (lift) of 5%</figcaption></figure><p>The first step is to calculate the required sample size with a power of 80% and minimum detectable effect of 5%. Let’s refer to this sample size requirement as the “exact” size. Then we divide and multiply it by three and call them “less” and “more”, respectively. For each sample size, we sample from the populations, compute the mean difference (typically called “lift”), and repeat this procedure many times.</p><p>The resulting distributions of lifts are shown below. The coloured vertical lines show the average lift of each experimental size, which is also equal to the actual population lift. This is <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\" rel=\"noopener\">central limit theorem</a> at work. What is interesting are the relative shapes of the sampling distributions. The “more” group is the safest and most consistent but the “exact” group seems to have good accuracy with only a third of the sample size. In the “less” group, we even see a non-negligible number of lifts that are negative, even though we know the population lift is actually +5%. This is in line with our intuition that larger sample size is generally more trustworthy than a smaller one, all else being equal.</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://miro.medium.com/max/1120/0*Jtrd9syTiZADm04j\" class=\"kg-image\"><figcaption><strong>Figure 3.</strong> Sampling distribution of the mean difference from each size group</figcaption></figure><p>Since we currently report the statistical significance of our experimental results at Gojek, we will also run t-test for each of our sample here.</p><p>As it turns out, 80.4% of the experiments in the “exact” group have statistically significant results (p &lt; 0.05), which is almost equal to the 80% power we set in the sample size calculator. In layman’s terms, the tool roughly promised that <em><em>if you run the experiment with this sample size, you have 80% chance of detecting a 5% lift (the minimum detectable effect) if it’s there.</em></em></p><p>Since it kept its promise even on non-normally distributed data, we can conclude that the methodology is robust to some degree of violation of the normality assumption. Therefore we can ignore it until otherwise proven.</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://miro.medium.com/max/1120/0*zF3ocylJM9iMEDue\" class=\"kg-image\"><figcaption><strong>Figure 4.</strong> Percentage of samples/experiments with statistically significant results (p &lt; 0.05) from each size group</figcaption></figure><p>Meanwhile, the “less” group had only ~34% statistically significant results, suggesting that we shouldn’t run an experiment if we cannot fulfil the sample size requirement — since we risk not being able to detect an existing effect. With the “more” group having close to 100% statistically significant results, we again reaffirm that larger sample size is generally a good thing if we can afford it.</p><p>Since it was deployed in mid-2018, many teams across Gojek have adopted the Sample Size Calculator. So far, nearly 1,000 experiments have incorporated this tool in their designs, and it is also a part of <a href=\"https://blog.gojekengineering.com/introducing-litmus-gojeks-own-experimentation-platform-3803467b6a53\" rel=\"noopener\">Litmus, our experimentation platform</a>. The more critical takeaway for us is that the experiments run using the recommended sample sizes have similar results when full-scaled. This fact has enabled us to iterate faster and cut waste while maintaining a high degree of confidence in the experimental results. ✌️</p><p>Did we mention Gojek’s Growth team is hiring analysts? We are a super data-driven team and have helped shape some of the company’s best practices through projects like this. If you are analytical, interested in sharpening your technical chops, and want to have a meaningful impact to Gojek’s hypergrowth, come join us!</p><hr><p>For more updates like this, don’t forget to <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">sign up for our newsletter</a>!</p>","url":"https://gojek-ghost.zysk.in/beast-moving-data-from-kafka-to-bigquery/","canonical_url":null,"uuid":"66b6dc18-3809-4494-a81e-a73d320935a3","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb0fef85524cd001e739246","reading_time":5}},{"node":{"id":"Ghost__Post__5eb0ff055524cd001e73924a","title":"Beast: Moving Data from Kafka to BigQuery","slug":"beast-moving-data-from-kafka-to-bigquery-2","featured":false,"feature_image":"https://res-2.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_dc5zljnB3Xzp17C7sZ3d-Q.jpg","excerpt":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","custom_excerpt":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"11 July, 2019","updated_at_pretty":"12 May, 2020","created_at":"2020-05-05T11:22:05.000+05:30","published_at":"2019-07-11T09:30:00.000+05:30","updated_at":"2020-05-12T11:53:53.000+05:30","meta_title":"Beast: Moving Data from Kafka to BigQuery","meta_description":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"by Rajat Goyal [http://medium.com/@rajat404] and Maulik Soneji\n[https://medium.com/@maulik.soneji]\n\nIn order to serve customers across 19+ products, GOJEK places a lot of emphasis\non data. Our Data Warehouse [https://en.wikipedia.org/wiki/Data_warehouse],\nbuilt by integrating data from multiple applications and sources, helps our team\nof data scientists, as well as business and product analysts make solid,\ndata-driven decisions.\n\nThis post explains our open source solution for easy movement of data from Kafka\nto BigQuery.\n\nData Warehouse setup at GOJEK\nWe use Google Bigquery [https://cloud.google.com/bigquery/] (BQ) as our Data\nWarehouse, which serves as a powerful tool for interactive analysis. This has\nproven extremely valuable for our use cases.\n\nOur approach to push data to our warehouse is to first push the data to Kafka.\nWe rely on multiple Kafka clusters to ingest relevant events across teams.\n\nA common approach to push data from Kafka to BigQuery is to first push it to\nGCS, and then import said data into BigQuery from GCS. While this solves the use\ncase of running analytics on historical data, we also use BigQuery for\nnear-real-time analytics & reporting. This analysis in-turn provides valuable\ninsights to make the right business decisions in a short time frame.\n\nThe initial approach\nOur original implementation used an individual code base for each topic in Kafka\nand use them to push data to BQ.\n\nThis required a lot of maintenance in order to keep up with the new topics and\nnew fields to existing topics being added to Kafka. Such changes required the\nmanual intervention of a dev/analyst to update the schema in both code and BQ\ntable. We also witnessed incidents of data loss on a few occasions, which\nrequired manually loading data from GCS.\n\nThe need for a new solution\nNew topics are added almost every other day to the several Kafka clusters in the\norganisation. Given that GOJEK has expanded its operations to several countries,\nmanaging the agglomeration of individual scripts for each topic was a massive\nordeal.\n\nIn order to deal with scaling issues, we decided to write a new system from\nscratch and take into account learnings from our previous experiences. Our\nsolution was a system that could ingest all the data pushed to Kafka and write\nit to Bigquery.\n\nWe decided to call it ‘Beast’ as it has to ingest all data that is generated in\nGOJEK.\n\nBefore starting with development, we had the following requirements to take care\nof:\n\n * No data loss: Every single message should be pushed from Kafka to BQ at least\n   once\n * Single codebase: A single repository able to handle any proto schema, for any\n   topic, without any code changes\n * Scalability: The app needs to be able to handle substantially high throughput\n * Observability: A dev should be able to see the state of the system at any\n   given point of time\n * Painless upgrades: Updating the schema for a topic should be a simple\n   operation\n\nArchitecture\n\nBeast takes inspiration from Sakaar\n[https://blog.gojekengineering.com/sakaar-taking-kafka-data-to-cloud-storage-at-go-jek-7839da20b5f3]\n, our in-house solution for pushing data from Kafka to GCS. Like Sakaar, Beast\nutilises Java’s blocking queues, for consuming, processing, pushing and\ncommitting the messages. Blocking queues allow us to make each of these stages\nindependent of the other, letting us optimise each stage in and of itself.\n\nEach Beast instance packs the following components:\n\n * Consumer: A native Kafka consumer, which consumes messages in batches from\n   Kafka, translates them to BQ compatible format, and pushes all of them into\n   two blocking queues — Read Queue and Commit Queue.\n * BQ Workers: A group of worker threads which pick messages from the read\n   queue, and push to BigQuery. Once a message batch is pushed successfully, the\n   BQ worker adds the committed offset of the batch to the Acknowledgement Set.\n   This offset acts as an acknowledgement of the batch being successfully\n   pushed.\n * Kafka Committer: A thread which keeps polling the head of the commit queue,\n   to get the earliest message batch. The committer looks for the commit offset\n   of that batch in the Acknowledgement Set. If the acknowledgement is available\n   (implying that the batch was successfully pushed to BQ), then the offset of\n   that batch is committed back to Kafka, and it’s removed from the commit\n   queue.\n\nSalient Features\nBeast is entirely cloud native, thus scaling it is a piece of cake.\n\nFor high throughput topics, all we need to do is spawn more pods. Since Beast\nrelies on Kafka consumers, we can have as many consumers as the number of\npartitions, and as long as they have the same consumer group, Kafka will ensure\nthat all the consumers receive unique messages.\n\nBeast takes a proto-descriptor file, which defines the details of all the protos\nin the registry. It then simply picks the details of the proto, specified in the\nconfiguration. This allows us to use the same codebase for all deployments, and\nalso makes the upgrades a breeze.\n\nBeast is open source ? ?\nBeast is now part of the open source domain. Do give it a shot!\n\nYou can find Beast here: ?\n\nhttps://github.com/gojek/beast\n\nHelm chart for the same can be found here\n[https://github.com/gojektech/charts/tree/master/incubator/beast].\n\nContributions, criticism, feedback, and bug reports are always welcome. ?\n\n\n--------------------------------------------------------------------------------\n\nIf you like what you read and want our stories delivered straight to your inbox, \nsign up for our newsletter [https://mailchi.mp/go-jek/gojek-tech-newsletter].","html":"<p>by <a href=\"http://medium.com/@rajat404\" rel=\"noopener\">Rajat Goyal</a> and <a href=\"https://medium.com/@maulik.soneji\" rel=\"noopener\">Maulik Soneji</a></p><p>In order to serve customers across 19+ products, GOJEK places a lot of emphasis on data. Our <a href=\"https://en.wikipedia.org/wiki/Data_warehouse\" rel=\"noopener\">Data Warehouse</a>, built by integrating data from multiple applications and sources, helps our team of data scientists, as well as business and product analysts make solid, data-driven decisions.</p><p>This post explains our open source solution for easy movement of data from Kafka to BigQuery.</p><h1 id=\"data-warehouse-setup-at-gojek\">Data Warehouse setup at GOJEK</h1><p>We use <a href=\"https://cloud.google.com/bigquery/\" rel=\"noopener\">Google Bigquery</a> (BQ) as our Data Warehouse, which serves as a powerful tool for interactive analysis. This has proven extremely valuable for our use cases.</p><p><em><em>Our approach to push data to our warehouse is to first push the data to Kafka. We rely on multiple Kafka clusters to ingest relevant events across teams.</em></em></p><p>A common approach to push data from Kafka to BigQuery is to first push it to GCS, and then import said data into BigQuery from GCS. While this solves the use case of running analytics on historical data, we also use BigQuery for near-real-time analytics &amp; reporting. This analysis in-turn provides valuable insights to make the right business decisions in a short time frame.</p><h1 id=\"the-initial-approach\">The initial approach</h1><p>Our original implementation used an individual code base for each topic in Kafka and use them to push data to BQ.</p><p>This required a lot of maintenance in order to keep up with the new topics and new fields to existing topics being added to Kafka. Such changes required the manual intervention of a dev/analyst to update the schema in both code and BQ table. We also witnessed incidents of data loss on a few occasions, which required manually loading data from GCS.</p><h1 id=\"the-need-for-a-new-solution\">The need for a new solution</h1><p>New topics are added almost every other day to the several Kafka clusters in the organisation. Given that GOJEK has expanded its operations to several countries, managing the agglomeration of individual scripts for each topic was a massive ordeal.</p><p>In order to deal with scaling issues, we decided to write a new system from scratch and take into account learnings from our previous experiences. Our solution was a system that could ingest all the data pushed to Kafka and write it to Bigquery.</p><p>We decided to call it ‘<strong>Beast’</strong> as it has to ingest all data that is generated in GOJEK.</p><p>Before starting with development, we had the following requirements to take care of:</p><ul><li><strong><strong>No data loss: </strong></strong>Every single message should be pushed from Kafka to BQ at least once</li><li><strong><strong>Single codebase: </strong></strong>A single repository able to handle any proto schema, for any topic, without any code changes</li><li><strong><strong>Scalability:</strong></strong> The app needs to be able to handle substantially high throughput</li><li><strong><strong>Observability:</strong></strong> A dev should be able to see the state of the system at any given point of time</li><li><strong><strong>Painless upgrades:</strong></strong> Updating the schema for a topic should be a simple operation</li></ul><h1 id=\"architecture\">Architecture<br></h1><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1373/1*MlBXLLD33_MwOyILGivskg.png\" class=\"kg-image\"></figure><p>Beast takes inspiration from <a href=\"https://blog.gojekengineering.com/sakaar-taking-kafka-data-to-cloud-storage-at-go-jek-7839da20b5f3\" rel=\"noopener\">Sakaar</a>, our in-house solution for pushing data from Kafka to GCS. Like Sakaar, Beast utilises Java’s blocking queues, for consuming, processing, pushing and committing the messages. Blocking queues allow us to make each of these stages independent of the other, letting us optimise each stage in and of itself.</p><p>Each Beast instance packs the following components:</p><ul><li><strong><strong>Consumer:</strong></strong> A native Kafka consumer, which consumes messages in batches from Kafka, translates them to BQ compatible format, and pushes all of them into two blocking queues — Read Queue and Commit Queue.</li><li><strong><strong>BQ Workers: </strong></strong>A group of worker threads which pick messages from the read queue, and push to BigQuery. Once a message batch is pushed successfully, the BQ worker adds the committed offset of the batch to the Acknowledgement Set. This offset acts as an acknowledgement of the batch being successfully pushed.</li><li><strong><strong>Kafka Committer:</strong></strong> A thread which keeps polling the head of the commit queue, to get the earliest message batch. The committer looks for the commit offset of that batch in the Acknowledgement Set. If the acknowledgement is available (implying that the batch was successfully pushed to BQ), then the offset of that batch is committed back to Kafka, and it’s removed from the commit queue.</li></ul><h1 id=\"salient-features\">Salient Features</h1><p>Beast is entirely cloud native, thus scaling it is a piece of cake.</p><p>For high throughput topics, all we need to do is spawn more pods. Since Beast relies on Kafka consumers, we can have as many consumers as the number of partitions, and as long as they have the same consumer group, Kafka will ensure that all the consumers receive unique messages.</p><p>Beast takes a proto-descriptor file, which defines the details of all the protos in the registry. It then simply picks the details of the proto, specified in the configuration. This allows us to use the same codebase for all deployments, and also makes the upgrades a breeze.</p><h1 id=\"beast-is-open-source-\">Beast is open source ? ?</h1><p>Beast is now part of the open source domain. Do give it a shot!</p><p><strong><strong>You can find Beast here: ?</strong></strong></p><p><a href=\"https://github.com/gojek/beast\">https://github.com/gojek/beast</a></p><p>Helm chart for the same can be found <a href=\"https://github.com/gojektech/charts/tree/master/incubator/beast\" rel=\"noopener\">here</a>.</p><p>Contributions, criticism, feedback, and bug reports are always welcome. ?</p><hr><p>If you like what you read and want our stories delivered straight to your inbox, <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\">sign up for our newsletter</a>. </p>","url":"https://gojek-ghost.zysk.in/beast-moving-data-from-kafka-to-bigquery-2/","canonical_url":null,"uuid":"3c91fa54-e0f7-43a2-88d8-55bf7fd56c9c","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb0ff055524cd001e73924a","reading_time":4}}]},"tags":{"edges":[{"node":{"name":"Culture","slug":"culture"}},{"node":{"name":"Data","slug":"data"}},{"node":{"name":"Design","slug":"design"}},{"node":{"name":"News","slug":"news"}},{"node":{"name":"Stories","slug":"stories"}},{"node":{"name":"Tech","slug":"tech"}},{"node":{"name":"Funding","slug":"funding"}},{"node":{"name":"Gojek","slug":"gojek"}},{"node":{"name":"Program Management","slug":"program-management"}},{"node":{"name":"Project Management","slug":"project-management"}},{"node":{"name":"Scrum","slug":"scrum"}},{"node":{"name":"Startup","slug":"startup"}}]}},"pageContext":{"slug":"data","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}}